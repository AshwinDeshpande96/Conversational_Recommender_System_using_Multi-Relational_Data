{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9e110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"entity_to_id\", \"id_to_entity\", \"entity_kg_clean\", \n",
    "        \"id_to_relation\", \"relation_to_id\", \"progress\", \n",
    "        \"new_id_map\", \"all_entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a33048",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp.pickle\", \"rb\") as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "    fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42827069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    data = {}\n",
    "    for key in keys:\n",
    "        data[key] = dataset[key].copy()\n",
    "#     print(data.keys())\n",
    "    with open(\"temp.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(data, fp)\n",
    "        fp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e316c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../dataset2.pickle\", \"rb\") as fp:\n",
    "#     dataset = pickle.load(fp)\n",
    "#     fp.close()\n",
    "# dataset[\"progress\"] = {}\n",
    "# dataset[\"new_id_map\"] = {}\n",
    "# dataset[\"all_entities\"] = list(dataset[\"entity_to_id\"].keys())\n",
    "# save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12274360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['entity_to_id', 'id_to_entity', 'entity_kg_clean', 'id_to_relation', 'relation_to_id', 'progress', 'new_id_map', 'all_entities'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4a3286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dbpedia_link(ent):\n",
    "    # time.sleep(1)\n",
    "    try:\n",
    "        obj = requests.get(\n",
    "            f\"https://lookup.dbpedia.org/api/search/KeywordSearch?format=JSON&QueryString={ent}\")\n",
    "        data = json.loads(obj.content)\n",
    "        for doc in data.get(\"docs\", []):\n",
    "            dbpedia_link = doc.get(\"resource\")\n",
    "            score = doc.get(\"score\")\n",
    "            if dbpedia_link and float(score[0]) > 2000:\n",
    "#                 print(ent, dbpedia_link, score)\n",
    "                return dbpedia_link[0]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87efa337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decompound_entities(compounded_entities, eid):\n",
    "    if eid not in dataset[\"new_id_map\"]:\n",
    "        dataset[\"new_id_map\"][eid] = set()\n",
    "    for ent in compounded_entities:\n",
    "        ent = ent.strip()\n",
    "        new_ent = get_dbpedia_link(ent)\n",
    "        if new_ent is not None:\n",
    "            if new_ent not in dataset[\"entity_to_id\"]:\n",
    "                new_eid = len(dataset[\"entity_to_id\"]) + 1\n",
    "                dataset[\"entity_to_id\"][new_ent] = new_eid\n",
    "                dataset[\"id_to_entity\"][new_eid] = new_ent\n",
    "            else:\n",
    "                new_eid = dataset[\"entity_to_id\"][new_ent]\n",
    "            dataset[\"new_id_map\"][eid].add(new_eid)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41417b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in tqdm(dataset[\"all_entities\"]):  \n",
    "    if entity in dataset[\"progress\"]:\n",
    "        continue\n",
    "    dataset[\"progress\"][entity] = 1\n",
    "    save()\n",
    "    eid = dataset[\"entity_to_id\"][entity]\n",
    "    if not isinstance(entity, str):\n",
    "        dataset[\"new_id_map\"][eid] = set()\n",
    "        continue\n",
    "    if re.match(\"[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\", entity):\n",
    "        dataset[\"new_id_map\"][eid] = set()\n",
    "        continue\n",
    "    if \"dbpedia\" in entity:\n",
    "        continue\n",
    "    if \",\" in entity:\n",
    "        decompound_entities(entity.split(\",\"), eid)\n",
    "    elif \"/\" in entity:\n",
    "        decompound_entities(entity.split(\"/\"), eid)\n",
    "    elif \"\\n\" in entity:\n",
    "        decompound_entities(entity.split(\"\\n\"), eid)\n",
    "    else:\n",
    "        decompound_entities([entity], eid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d9f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 57374/57374\n",
      "31301\n"
     ]
    }
   ],
   "source": [
    "print(f\"Completed: {len(dataset['progress'])}/{len(dataset['all_entities'])}\")\n",
    "print(len(dataset[\"new_id_map\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1ce745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "for eid in dataset[\"new_id_map\"]:\n",
    "    entity = dataset[\"id_to_entity\"].pop(eid)\n",
    "    dataset[\"entity_to_id\"].pop(entity)\n",
    "################################\n",
    "num_edges = len(dataset[\"entity_kg_clean\"])\n",
    "for i in range(num_edges):\n",
    "    e1id, rid, e2id = dataset[\"entity_kg_clean\"][i]\n",
    "    new_e1s = set([e1id])\n",
    "    if e1id in dataset[\"new_id_map\"]:\n",
    "        new_e1s = dataset[\"new_id_map\"][e1id]\n",
    "    new_e2s = set([e2id])\n",
    "    if e2id in dataset[\"new_id_map\"]:\n",
    "        new_e2s = dataset[\"new_id_map\"][e2id]\n",
    "    dataset[\"entity_kg_clean\"][i] = None\n",
    "    for new_e1 in new_e1s:\n",
    "        for new_e2 in new_e2s:\n",
    "            dataset[\"entity_kg_clean\"].append((new_e1, rid, new_e2))\n",
    "dataset[\"entity_kg_clean\"] = [edge for edge in dataset[\"entity_kg_clean\"] if edge is not None]\n",
    "#####################################\n",
    "# for edge_id in edges_to_be_removed:\n",
    "#     dataset[\"entity_kg_clean\"].pop(edge_id)\n",
    "\n",
    "# for eid in entities_to_be_removed:\n",
    "#     entity = dataset[\"id_to_entity\"][eid]\n",
    "#     dataset[\"id_to_entity\"].pop(eid)\n",
    "#     dataset[\"entity_to_id\"].pop(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "770715b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2-True\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "57378",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28308\\1285688432.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"e2-True\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0me1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id_to_entity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me1id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0me2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id_to_entity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me2id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id_to_relation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentity_to_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 57378"
     ]
    }
   ],
   "source": [
    "entity_to_id = {}\n",
    "id_to_entity = {}\n",
    "relation_to_id = {}\n",
    "id_to_relation = {}\n",
    "for i in range(len(dataset[\"entity_kg_clean\"])):\n",
    "    e1id, rid, e2id = dataset[\"entity_kg_clean\"][i]\n",
    "    if e1id in dataset[\"new_id_map\"]:\n",
    "        print(\"e1-True\")\n",
    "    if e2id in dataset[\"new_id_map\"]:\n",
    "        print(\"e2-True\")\n",
    "    e1 = dataset[\"id_to_entity\"][e1id]\n",
    "    e2 = dataset[\"id_to_entity\"][e2id]\n",
    "    r = dataset[\"id_to_relation\"][rid]\n",
    "    if e1 in entity_to_id:\n",
    "        new_e1id = entity_to_id[e1]\n",
    "    else:\n",
    "        new_e1id = len(entity_to_id) + 1\n",
    "        entity_to_id[e1] = new_e1id\n",
    "        id_to_entity[new_e1id] = e1\n",
    "    \n",
    "    if e2 in entity_to_id:\n",
    "        new_e2id = entity_to_id[e2]\n",
    "    else:\n",
    "        new_e2id = len(entity_to_id) + 1\n",
    "        entity_to_id[e2] = new_e2id\n",
    "        id_to_entity[new_e2id] = e2\n",
    "        \n",
    "    if r in relation_to_id:\n",
    "        new_rid = relation_to_id[r]\n",
    "    else:\n",
    "        new_rid = len(relation_to_id) + 1\n",
    "        relation_to_id[r] = new_rid\n",
    "        id_to_relation[new_rid] = r  \n",
    "    dataset[\"entity_kg_clean\"][i] = (new_e1id, new_rid, new_e2id)\n",
    "dataset[\"entity_to_id\"] = word_to_id\n",
    "dataset[\"id_to_entity\"] = id_to_word\n",
    "dataset[\"relation_to_id\"] = relation_to_id\n",
    "dataset[\"id_to_relation\"] = id_to_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291f1ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open(\"../dataset_clean.pickle\", \"wb\") as fp:\n",
    "#     pickle.dump(dataset, fp)\n",
    "#     fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365081d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chat]",
   "language": "python",
   "name": "conda-env-chat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
